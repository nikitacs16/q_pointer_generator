#data_paths
train_data_path: ''
dev_data_path: ''
val_data_path: ''
vocab_path: ''

#output paths
log_root: '' #Root directory for all logging.
exp_name: '' #Name for experiment. Logs will be saved in a directory with this name, under log_root.

#Hyperparameters
hidden_dim: 200 #dimension of RNN hidden states
emb_dim: 128 #dimension of word embeddings
batch_size: 16  #minibatch size'
max_enc_steps: 500 #max timesteps of encoder (max source text tokens)
max_que_steps: 100 #max timesteps of query encoder (max source query tokens)
max_dec_steps: 60 #max timesteps of decoder (max summary tokens)
beam_size: 4  #beam size for beam search decoding.
min_dec_steps: 3 #Minimum sequence length of generated summary. Applies only for beam search decoding mode
vocab_size: 25000  #'Size of vocabulary. These will be read from the vocabulary file in order. If the vocabulary file contains fewer words than this number, or if this number is set to 0, will take all words in the vocabulary file.
lr: 0.15 #learning rate
adagrad_init_acc: 0.01 #initial accumulator value for Adagrad
rand_unif_init_mag: 0.02 #magnitude for lstm cells random uniform inititalization
trunc_norm_init_std: 1e-4 #std of trunc norm init, used for initializing everything else
max_grad_norm: 2.0 #for gradient clipping

#other_options
pointer_gen: True #If True, use pointer-generator model. If False, use baseline model.
max_to_keep: 10
early_stopping_steps: 12000
